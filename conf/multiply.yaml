defaults:
    - base
    - _self_
finetune:
    seq_length: 4000
    gradient_accumulation_passes: 1024
    save_checkpoint_steps: 1000
llm:
    parameters:
        max_tokens: 500
test_llm:
    parameters:
        max_tokens: 500
vllm_config:
    vllm_kwargs:
        max_model_len: 4000  # Match seq_length for multiply task
actor:
  rollout_policy: pipelinerl.domains.puzzle.multiply.generate_multiply_rollout
  system_prompt: null
  task_template: null

environment: null

dataset_loader: pipelinerl.domains.puzzle.multiply.load_multiply_problems
train_dataset_names:
  - train
test_dataset_names:
  - test

world:
  actor_fraction: 1    # 1 GPU for vLLM inference (sampling)
  preprocessor_fraction: 0
  finetune_fraction: 1  # 1 GPU for training

eval_every_n_versions: 20000
model_path: Qwen/Qwen2.5-0.5B-Instruct